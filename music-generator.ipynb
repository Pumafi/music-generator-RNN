{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/ferdi/.local/lib/python3.8/site-packages (2.3.1)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /home/ferdi/.local/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/ferdi/.local/lib/python3.8/site-packages (from tensorflow) (0.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/ferdi/.local/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/ferdi/.local/lib/python3.8/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/ferdi/.local/lib/python3.8/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ferdi/.local/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /home/ferdi/.local/lib/python3.8/site-packages (from tensorflow) (1.18.5)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /home/ferdi/.local/lib/python3.8/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/ferdi/.local/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/ferdi/.local/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /home/ferdi/.local/lib/python3.8/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/ferdi/.local/lib/python3.8/site-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ferdi/.local/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/ferdi/.local/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from protobuf>=3.9.2->tensorflow) (45.2.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ferdi/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ferdi/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ferdi/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.21.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ferdi/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ferdi/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ferdi/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /home/ferdi/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ferdi/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ferdi/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ferdi/.local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: music21 in /home/ferdi/.local/lib/python3.8/site-packages (6.3.0)\n",
      "Requirement already satisfied: more-itertools in /usr/lib/python3/dist-packages (from music21) (4.2.0)\n",
      "Requirement already satisfied: joblib in /home/ferdi/.local/lib/python3.8/site-packages (from music21) (0.16.0)\n",
      "Requirement already satisfied: webcolors in /home/ferdi/.local/lib/python3.8/site-packages (from music21) (1.11.1)\n",
      "Requirement already satisfied: chardet in /usr/lib/python3/dist-packages (from music21) (3.0.4)\n",
      "Requirement already satisfied: tqdm in /home/ferdi/.local/lib/python3.8/site-packages (4.54.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow\n",
    "!pip3 install music21\n",
    "!pip3 install tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Layer\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from music21 import converter, abcFormat, midi\n",
    "import time\n",
    "import functools\n",
    "from IPython import display as ipythondisplay\n",
    "from tqdm import tqdm\n",
    "\n",
    "!apt-get install abcmidi timidity timidity-interfaces-extra freepats > /dev/null 2>&1\n",
    "\n",
    "#assert len(tf.config.list_physical_devices('GPU')) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: ./nottingham_database/nottingham.abc: input file is output file\r\n"
     ]
    }
   ],
   "source": [
    "import requests, zipfile, io\n",
    "r = requests.get(\"http://ifdo.ca/~seymour/nottingham/nottingham_database.zip\")\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall(\"./\")\n",
    "!cat ./nottingham_database/*.abc > ./nottingham_database/nottingham.abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"./nottingham_database/nottingham.abc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 6\n",
      "T:Bonnie Kate\n",
      "S:Mick Peat\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G\n",
      "f|\"G\"g3/2a/2 \"D\"gf|\"G\"gd2e|\"C\"dc \"D\"BA|\"G\"BG2A|\"G\"B\" Em\"G2A/2B/2|\\\n",
      "\"Am\"c\" D\"A2B/2c/2|\"G\"Bd \"C\"cB| [1\"D\"Ad ef:|\n",
      " [2\"D\"A2 A2||\"G\"BG2A/2B/2|\"D\"cA \"G7\"dB|\"C\"ec2d/2e/2|\"D\"fzde/2f/2|\n",
      "\"G\"g3/2a/2 \"Em\"ge|\"Bm\"df \"Em\"gB|\"Am\"ce \"D\"d/2e/2d/2c/2|\"G\"BG G2:|\n"
     ]
    }
   ],
   "source": [
    "with open(dataset) as f:\n",
    "    musics_string = re.sub(r'(?m)^\\%.*\\n?', '', f.read())\n",
    "    musics_string = re.sub(r'(?m)^F: http.*\\n?', '', musics_string)\n",
    "    musics = re.split('\\n\\n+', musics_string)\n",
    "    \n",
    "print(musics[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "abcScore = converter.parse(musics[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play test music\n",
    "#sp = midi.realtime.StreamPlayer(abcScore)\n",
    "#sp.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "musical_alphabet = sorted(set(musics_string))\n",
    "print(len(musical_alphabet))\n",
    "\n",
    "char_to_scal =  {u:i for i, u in enumerate(musical_alphabet)}\n",
    "scal_to_char = list(musical_alphabet)\n",
    "\n",
    "def vectorize_string(string):\n",
    "  return np.array([char_to_scal[c] for c in string])\n",
    "\n",
    "vectorized_songs = vectorize_string(musics_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random_batch(seq_len = 5, batch_size = 1):\n",
    "    inputs = [vectorized_songs[i:i + seq_len] for i in range(len(musics_string) - seq_len)]\n",
    "    outputs = [vectorized_songs[i:i + seq_len] for i in range(1, len(musics_string) - seq_len + 1)]\n",
    "    batch_indexes = np.random.choice(len(inputs), batch_size)\n",
    "    batch_input = np.reshape([inputs[i] for i in batch_indexes], [batch_size, seq_len])\n",
    "    batch_output = np.reshape([outputs[i] for i in batch_indexes], [batch_size, seq_len])\n",
    "    return batch_input, batch_output\n",
    "\n",
    "batch_input, batch_output = make_random_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31 89  0  3 34]]\n",
      "[[89  0  3 34  3]]\n"
     ]
    }
   ],
   "source": [
    "print(batch_input[:5])\n",
    "print(batch_output[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_LSTM(rnn_units): \n",
    "      return tf.keras.layers.LSTM(\n",
    "        rnn_units, \n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        recurrent_activation='sigmoid',\n",
    "        stateful=True,\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class musicGenRNN(Model):\n",
    "    def __init__(self, musical_alphabet_size, embedding_dim, rnn_units, batch_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = Embedding(musical_alphabet_size, embedding_dim, batch_input_shape=[batch_size, None])\n",
    "        self.lstm = my_LSTM(rnn_units)\n",
    "        self.classifier = Dense(musical_alphabet_size)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.lstm(x)\n",
    "        y = self.classifier(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"music_gen_rnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  23552     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  94300     \n",
      "=================================================================\n",
      "Total params: 5,364,828\n",
      "Trainable params: 5,364,828\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_len=10\n",
    "batch_size=32\n",
    "\n",
    "input_shape = [len(musical_alphabet)]\n",
    "music_gen_rnn = musicGenRNN(len(musical_alphabet), embedding_dim=256, rnn_units=1024, batch_size=batch_size)\n",
    "music_gen_rnn.build((batch_size, *input_shape))\n",
    "music_gen_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_random_batch(seq_len, batch_size)\n",
    "pred = music_gen_rnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_iterations = 2000 \n",
    "batch_size = 10\n",
    "seq_length = 100\n",
    "learning_rate = 1e-3 \n",
    "\n",
    "# Model parameters: \n",
    "musical_alphabet_size = len(musical_alphabet)\n",
    "embedding_dim = 256 \n",
    "rnn_units = 1024  # Experiment between 1 and 2048\n",
    "\n",
    "# Checkpoint location: \n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"music_gen_rnn_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  23552     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  94300     \n",
      "=================================================================\n",
      "Total params: 5,364,828\n",
      "Trainable params: 5,364,828\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "music_gen_rnn = musicGenRNN(musical_alphabet_size, embedding_dim, rnn_units, batch_size)\n",
    "music_gen_rnn.build((batch_size, *input_shape))\n",
    "music_gen_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(labels, logits):\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "    return loss\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 54/2000 [03:12<1:48:27,  3.34s/it]"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        y_pred = music_gen_rnn(x)\n",
    "        loss = compute_loss(y, y_pred)\n",
    "        grads = tape.gradient(loss, music_gen_rnn.trainable_variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, music_gen_rnn.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "##################\n",
    "# Begin training!#\n",
    "##################\n",
    "\n",
    "history = []\n",
    "\n",
    "for iter in tqdm(range(num_training_iterations)):\n",
    "    x_batch, y_batch = make_random_batch(seq_length, batch_size)\n",
    "    loss = train_step(x_batch, y_batch)\n",
    "\n",
    "    # Update the progress bar\n",
    "    history.append(loss.numpy().mean())\n",
    "    plt.plot(history)\n",
    "\n",
    "    # Update the model with the changed weights!\n",
    "    if iter % 100 == 0:\n",
    "        music_gen_rnn.save_weights(checkpoint_prefix)\n",
    "    \n",
    "# Save the trained model and the weights\n",
    "music_gen_rnn.save_weights(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_gen_rnn = musicGenRNN(len(musical_alphabet), embedding_dim=256, rnn_units=1024, batch_size=1)\n",
    "music_gen_rnn.build((1, *input_shape))\n",
    "# Restore the model weights for the last checkpoint after training\n",
    "music_gen_rnn.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "music_gen_rnn.build(tf.TensorShape([1, None]))\n",
    "\n",
    "music_gen_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_new_music(model, start_string, music_length):\n",
    "    vectorised_input = np.array([char_to_scal[c] for c  in start_string])\n",
    "    text_generated = []\n",
    "    \n",
    "    model.reset_states()\n",
    "    \n",
    "    vectorised_input = tf.expand_dims(vectorised_input, 0)\n",
    "    \n",
    "    for i in range(music_length):\n",
    "        y_pred = model(vectorised_input)\n",
    "        y_pred = tf.squeeze(y_pred, 0)\n",
    "        y_id = tf.random.categorical(y_pred, 1)[-1,0].numpy()\n",
    "        vectorised_input = tf.expand_dims([y_id], 0)\n",
    "        text_generated.append(scal_to_char[y_id])\n",
    "    \n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music = predict_new_music(music_gen_rnn, 'X', 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play test music\n",
    "abcScore = converter.parse(music)\n",
    "sp = midi.realtime.StreamPlayer(abcScore)\n",
    "sp.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
